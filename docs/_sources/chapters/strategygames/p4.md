---
jupytext:
  text_representation:
    format_name: myst
kernelspec:
  name: python3
  display_name: Python 3
---

# Problem 4: The Roll-Take Conundrum

```{note}
This problem requires some programming experience/environment
```

Consider a game involving a fair $k$-sided die with faces labeled $1$ through $k$, initially showing a uniformly at random face. You are allowed to perform $A$ actions in total. At each step, you may choose between the following two actions:

Roll: Re-roll the die to generate a new face uniformly at random from $\{1, \dots, k\}$.

Take: Accept the current face value as a payout (equal to that face value), then immediately roll the die again. You may not take twice in a row; a roll must occur between any two takes.

A natural strategy is to fix a threshold $n \in \{1, \dots, k\}$ in advance and to accept (take) the value currently showing if and only if it is at least $n$.

Assuming optimal play, what $n$ should you choose for $k = 20$, $A = 100$?

````{dropdown} Click to show solution

We fix a threshold $n$ and analyze the expected payout under the threshold strategy: take the current value if and only if it is at least $n$. Let $E(t)$ denote the expected total payout with $t$ actions remaining, given that the die shows a uniformly random value due to previous rolls.

If $t \geq 2$, each **take** uses **2 actions**: one to take and one mandatory roll afterward. A **roll** uses 1 action.

Assume that at a given state the die shows a uniformly random value due to previous rolls. Then:

- With probability $\dfrac{k - n + 1}{k}$, the current die value is $\geq n$, and we choose to **take**. This consumes 2 actions (since we have to roll after taking), and gives expected payout 

  $$
  \mathbb{E}[\text{value} \mid \geq n] = \frac{n + k}{2},
  $$
  and then we're left with $t - 2$ actions.

- With probability $\dfrac{n - 1}{k}$, the current value is $< n$, and we **roll** again, costing 1 action and leaving $t - 1$ actions.

Therefore, for $t \geq 2$, the recurrence is:

$$
E(t) = \frac{n - 1}{k} E(t - 1) + \frac{k - n + 1}{k} \left( \frac{n + k}{2} + E(t - 2) \right).
$$

The base cases are:

$$
E(0) = 0, \quad E(1) = \frac{k+1}{2}.
$$

And our answer is $E(A)$. Unfortunately, this recurrence has no clean solution.

---

**Asymptotic Behavior**

When $A$ is large, we expect the average reward per action to converge to some constant depending on $k$ and $n$. We can approximate this as:

$$
\text{Average reward} \approx \frac{\text{number of actions}}{\text{expected number of actions per decision}} \cdot \text{expected gain per take}.
$$

More precisely:

$$
\text{Average reward} \approx \frac{A}{ \frac{k}{k-n+1} + 1 } \cdot  \frac{n + k}{2}.
$$

This expression can be maximized over $n$ to get the asymptotically optimal strategy by differentiating with respect to $n$, and setting the derivative to zero to get:

$$
n = 2k + 1 - \sqrt{3k^2 + k}.
$$

In our case, for $k = 20,\; A = 100$, this gives $n \approx 6$, and reward $\approx \frac{3900}{7} \approx 557.143$. However, as noted above, this is simply an asymptotic behavior and not an exact solution.

---

**Implementation Strategy**

For fixed $k$ and $A$, we compute $E(t)$ for $t = 0$ to $t = A$ for each threshold $n = 1$ to $k$ using dynamic programming, and then select the value of $n$ that maximizes $E(A)$. Below is Python code to do this. 

````

```{code-cell} python
:tags: [hide-input, hide-output]
k = 20  # Number of faces
A = 100  # Number of actions available
best_reward = float('-inf')
best_n = None

# Try each possible threshold n from 1 to k
for n in range(1, k + 1):
    # Initialize E(0) = 0, E(1) = (k+1)/2 as base cases
    E_prev2 = 0
    E_prev1 = (k + 1) / 2

    # Compute E(A) using recurrence relation
    for _ in range(A - 1):
        E_prev2, E_prev1 = (
            E_prev1,
            ((n - 1) / k) * E_prev1 + ((k - n + 1) / k) * ((n + k) / 2 + E_prev2),
        )

    # Update best threshold if this one gives better expected reward
    if E_prev1 > best_reward:
        best_reward = E_prev1
        best_n = n

print(f"Best reward is at n={best_n} and reward={best_reward:.2f}")
```
