# Problem 6: Revenge of the QR

```{note}
You should attempt [](p4.md) and [](p5.md) before attempting this problem. 
```

Suppose that you have two matrices of regressors $X_1$ and $X_2$ where:

- $X_1$ is $N \times k$
- $X_2$ is $N \times (f - k)$

You fit a multiple regression model for a target variable $Y$ onto $X_1$, obtaining residuals $e$, and a QR decomposition of the matrix $X_1$.

---

Derive an efficient procedure to compute the new QR decomposition of the regression using all variables in $X_1$ plus the variable from $X_2$ you want to add based on [](p4.md) and [](p5.md).

````{dropdown} Click to show solution

**Objective**  
Given the QR decomposition of $X_1 \in \mathbb{R}^{N \times k}$, specifically $X_1 = Q_1 R_1$ with $Q_1^\top Q_1 = I_k$, and having chosen a new predictor $x_j$ to add (as determined in [](p4.md)), we wish to efficiently compute the QR decomposition of the augmented design matrix:

$$
X_{\text{new}} = \begin{bmatrix} X_1 & x_j \end{bmatrix} \in \mathbb{R}^{N \times (k+1)}.
$$

We aim to avoid recomputing QR from scratch, exploiting what we already know about $X_1$.

---

**Step 1: Geometric viewpoint â€” orthogonalizing $x_j$**

From [](p4.md), recall that $Q_1$ forms an orthonormal basis for $\mathrm{col}(X_1)$. Therefore, any new vector $x_j$ can be decomposed uniquely into:

$$
x_j = Q_1 Q_1^\top x_j \;+\; z_j,
$$

where:

- $Q_1 Q_1^\top x_j$ is the projection of $x_j$ onto $\mathrm{col}(X_1)$,
- $z_j = (I - Q_1 Q_1^\top) x_j$ is the component of $x_j$ orthogonal to $\mathrm{col}(X_1)$.

This decomposition is crucial: the "new" information in $x_j$ comes entirely from $z_j$.  
If $z_j = 0$, $x_j$ brings no new direction, and the augmented matrix is rank-deficient.

---

**Step 2: Updating the Q matrix (enlarging the orthonormal basis)**

If $z_j \neq 0$, we can extend $Q_1$ to a larger orthonormal basis by appending:

$$
q_{k+1} = \frac{z_j}{\| z_j \|_2}.
$$

This ensures:

- $q_{k+1} \perp Q_1$,
- $\| q_{k+1} \|_2 = 1$.

We now define:

$$
Q_{\text{new}} = \begin{bmatrix} Q_1 & q_{k+1} \end{bmatrix} \in \mathbb{R}^{N \times (k+1)}.
$$

This new matrix $Q_{\text{new}}$ still satisfies:

$$
Q_{\text{new}}^\top Q_{\text{new}} = I_{k+1}.
$$

Thus, we have extended our orthonormal basis to accommodate $x_j$.

---

**Step 3: Updating the R matrix (upper-triangular factor)**

We now express $x_j$ in the newly extended basis:

- The projection of $x_j$ onto $\mathrm{col}(X_1)$ is already known as $Q_1 Q_1^\top x_j$.
- The coordinates of this projection in $Q_1$ are simply:

  $$
  w = Q_1^\top x_j \in \mathbb{R}^k.
  $$

- The coefficient of $x_j$ in the new orthogonal direction $q_{k+1}$ is its norm:

  $$
  r_{k+1,k+1} = \| z_j \|_2.
  $$

Thus, the updated $R$ matrix is:

$$
R_{\text{new}} =
\begin{pmatrix}
R_1 & w \\
0 & r_{k+1,k+1}
\end{pmatrix} \in \mathbb{R}^{(k+1) \times (k+1)}.
$$

This is consistent with QR: $X_{\text{new}} = Q_{\text{new}} R_{\text{new}}$.

---

**Step 4: Why is this efficient?**

- Computing $w = Q_1^\top x_j$ takes $O(Nk)$ time.
- Computing $z_j = x_j - Q_1 w$ is $O(Nk)$.
- Computing $\| z_j \|_2$ is $O(N)$.
- Constructing $q_{k+1}$ is $O(N)$.

In total, we avoid recomputing a full QR factorization, and instead update with $O(Nk)$ work.

---

**Conclusion**  
By orthogonalizing $x_j$ against the existing basis $Q_1$ and expanding the $Q$ and $R$ matrices accordingly, we efficiently update the QR decomposition to include the new variable chosen in [](p4.md), without starting over. Together with [](p5.md) gives a method to update both the QR decomposition and $\beta$. 

This is the standard Gram-Schmidt step reused with care.

````
