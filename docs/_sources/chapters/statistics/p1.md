# Problem 1: Episode I: The Weighted Average Awakens

Consider two independent OLS regressions without intercepts:

$$
Y_1 = \beta_1 X_1 + \varepsilon_1, \quad \text{where} \quad Y_1, X_1 \in \mathbb{R}^{M},
$$

$$
Y_2 = \beta_2 X_2 + \varepsilon_2, \quad \text{where} \quad Y_2, X_2 \in \mathbb{R}^{N}.
$$

Now consider the combined dataset formed by concatenating:

- $Y = \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix} \in \mathbb{R}^{M+N}$,
- $X = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} \in \mathbb{R}^{M+N}$,

and fit a third OLS model (without intercept):

$$
Y = \beta X + \varepsilon.
$$

Given the values of $\beta_1$ and $\beta_2$, determine the possible range of values that the combined regression coefficient $\beta$ can take, as a function of $\beta_1$, $\beta_2$.

````{dropdown} Click to show solution

**One-Dimensional OLS Without Intercept.**  
Let $X \in \mathbb{R}^N$ and $Y \in \mathbb{R}^N$. The ordinary least-squares estimator $\beta^*$ is defined by

$$
\beta^* = \arg\min_{\beta \in \mathbb{R}} \|Y - \beta X\|_2^2.
$$

Expanding the objective gives

$$
\|Y - \beta X\|_2^2 = Y^\top Y - 2\beta X^\top Y + \beta^2 X^\top X.
$$

Differentiating with respect to $\beta$ and setting the derivative to zero,

$$
\frac{d}{d\beta} \left( Y^\top Y - 2\beta X^\top Y + \beta^2 X^\top X \right)
= -2 X^\top Y + 2\beta X^\top X = 0,
$$

we obtain

$$
\beta^* = \frac{X^\top Y}{X^\top X}.
$$

In the multivariate case, where $X \in \mathbb{R}^{N \times p}$, a similar calculation yields the familiar formula

$$
\beta^* = (X^\top X)^{-1} X^\top Y.
$$

---

**Pooled Estimate from Two Samples.**  
Suppose we have two disjoint datasets $(X_1, Y_1)$ of size $M$ and $(X_2, Y_2)$ of size $N$, each with its own one-dimensional regressor and response. Their individual OLS estimators (without intercept) are

$$
\beta_1 = \frac{X_1^\top Y_1}{X_1^\top X_1},
\qquad
\beta_2 = \frac{X_2^\top Y_2}{X_2^\top X_2}.
$$

Form the combined dataset by concatenation:

$$
X = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} \in \mathbb{R}^{(M+N) \times 1},
\qquad
Y = \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix} \in \mathbb{R}^{(M+N) \times 1}.
$$

The pooled estimator is

$$
\beta = \frac{X^\top Y}{X^\top X}
= \frac{X_1^\top Y_1 + X_2^\top Y_2}{X_1^\top X_1 + X_2^\top X_2}
= \frac{\beta_1 X_1^\top X_1 + \beta_2 X_2^\top X_2}{X_1^\top X_1 + X_2^\top X_2}.
$$

Because $X_1^\top X_1$ and $X_2^\top X_2$ are nonnegative weights, it follows that

$$
\min\{\beta_1, \beta_2\} \leq \beta \leq \max\{\beta_1, \beta_2\}.
$$

Moreover, this inclusion is tight when $\beta_1 = \beta_2$. Hence

$$
\boxed{ \beta \in \left[ \min(\beta_1, \beta_2), \; \max(\beta_1, \beta_2) \right] }.
$$

````
