# Problem 4: The Variable Menace

Suppose that you have two matrices of regressors $X_1$ and $X_2$ where:

- $X_1$ is $N \times k$
- $X_2$ is $N \times (f - k)$

You fit a multiple regression model for a target variable $Y$ onto $X_1$, obtaining residuals $e$, and a QR decomposition of the matrix $X_1$.

---

You want to find which one of the additional variables contained in $X_2$ will reduce the residual sum of squares (RSS) the most when included with the ones from $X_1$.

Detail an efficient procedure to obtain this.

````{dropdown} Click to show solution

**QR factorization and residual orthogonality**  
Write the current design matrix $X_1 \in \mathbb{R}^{N \times k}$ in its reduced QR form:

$$
X_1 = Q_1 R_1,
\quad
Q_1 \in \mathbb{R}^{N \times k},\;
R_1 \in \mathbb{R}^{k \times k},
\quad
Q_1^\top Q_1 = I_k.
$$

Fitting $Y \in \mathbb{R}^N$ by OLS on $X_1$ yields coefficients $\beta_1$ and residual

$$
e = Y - X_1 \beta_1.
$$

By the normal-equations property of OLS, $X_1^\top e = 0$, i.e.

$$
e \perp \mathrm{col}(X_1).
$$

---

**Decomposing a new predictor**  
For each candidate predictor $x_j \in \mathbb{R}^N$ (the $j$-th column of $X_2 \in \mathbb{R}^{N \times (f-k)}$), decompose it into its projection onto $\mathrm{col}(X_1)$ and its orthogonal complement:

$$
x_j = Q_1 Q_1^\top x_j + z_j,
\quad
z_j := (I - Q_1 Q_1^\top) x_j.
$$

By construction, $z_j \perp \mathrm{col}(X_1)$.

---

**Regression of the residual on $z_j$**  
Since $e \perp Q_1 Q_1^\top x_j$, regressing $e$ on $x_j$ is equivalent to regressing on $z_j$ alone. Consider the one-dimensional fit:

$$
e = \gamma \, z_j,
$$

The minimizer is:

$$
\gamma_j = \arg\min_\gamma \|e - \gamma z_j\|_2^2 = \frac{e^\top z_j}{z_j^\top z_j},
$$

giving residual sum of squares:

$$
\|e - \gamma_j z_j\|_2^2 = e^\top e - \frac{(e^\top z_j)^2}{z_j^\top z_j}.
$$

Hence the decrease in RSS from adding $x_j$ is:

$$
\Delta_j = \frac{(e^\top z_j)^2}{z_j^\top z_j}.
$$

---

**Simplifying numerator and denominator**  
First, because $e \perp Q_1$:

$$
e^\top z_j = e^\top (x_j - Q_1 Q_1^\top x_j) = e^\top x_j.
$$

Next, expand:

$$
\|z_j\|^2 = z_j^\top z_j = x_j^\top x_j - 2 x_j^\top Q_1 Q_1^\top x_j + (Q_1 Q_1^\top x_j)^\top (Q_1 Q_1^\top x_j).
$$

Since $Q_1^\top Q_1 = I$, this simplifies to:

$$
\|z_j\|^2 = \|x_j\|_2^2 - \|Q_1^\top x_j\|_2^2.
$$

Therefore:

$$
\Delta_j = \frac{(e^\top x_j)^2}{\|x_j\|_2^2 - \|Q_1^\top x_j\|_2^2}.
$$

---

**Algorithmic implementation**  
Given $Q_1$, $\beta_1$, and $X_2 = [x_1, \dots, x_{f-k}]$:

1. Compute the residual:

   $$
   e = Y - X_1 \beta_1.
   $$

2. For each $j = 1, \dots, f-k$, compute:

   $$
   A_j = e^\top x_j,
   \quad
   B_j = \|x_j\|_2,
   \quad
   C_j = \|Q_1^\top x_j\|_2.
   $$

3. Form the RSS-reduction score:

   $$
   \Delta_j = \frac{A_j^2}{B_j^2 - C_j^2}.
   $$

4. Select:

   $$
   j^* = \arg\max_{1 \leq j \leq f-k} \Delta_j.
   $$

The predictor $x_{j^*}$ maximizes the reduction in the residual sum of squares when appended to the model on $X_1$.

````
