---
jupytext:
  text_representation:
    format_name: myst
kernelspec:
  name: python3
  display_name: Python 3
---
# Problem 2: Episode II: Attack of the Intercepts

```{note}
You should attempt [](p1.md) before attempting this problem. 
```

Consider two independent OLS regressions **with** intercepts:

$$
Y_1 = \alpha_1 + \beta_1 X_1 + \varepsilon_1, \quad \text{where} \quad Y_1, X_1 \in \mathbb{R}^{M},
$$

$$
Y_2 = \alpha_2 + \beta_2 X_2 + \varepsilon_2, \quad \text{where} \quad Y_2, X_2 \in \mathbb{R}^{N}.
$$

Now consider the combined dataset formed by concatenating:

- $Y = \begin{bmatrix} Y_1 \\ Y_2 \end{bmatrix} \in \mathbb{R}^{M+N}$,
- $X = \begin{bmatrix} X_1 \\ X_2 \end{bmatrix} \in \mathbb{R}^{M+N}$,

and fit a third OLS model (**with** intercept):

$$
Y = \alpha + \beta X + \varepsilon.
$$

Given the values of $\beta_1$ and $\beta_2$, determine the possible range of values that the combined regression coefficient $\beta$ can take, as a function of $\beta_1$, $\beta_2$.

````{dropdown} Click to show solution

*Generally speaking, the pooled slope can be made arbitrarily large (or small), or*  

$$
\boxed{\beta \in (-\infty, \infty)}.
$$

---

**Setup and assumptions**

Let $(X_1, Y_1)$ of size $M$ and $(X_2, Y_2)$ of size $N$ be two one-dimensional datasets. Form the concatenated data

$$
X = \begin{pmatrix} X_1 \\ X_2 \end{pmatrix},
\quad
Y = \begin{pmatrix} Y_1 \\ Y_2 \end{pmatrix}.
$$

We make two mild assumptions that lose no generality:

1. The feature-means differ: $\bar X_1 \neq \bar X_2$. (If they were equal, one may translate one block of $X$s by a constant without changing its slope.)

2. The pooled $X$ has nonzero variance (i.e., is not all the same value), so the OLS with intercept is well-defined. (Otherwise it is degenerate.)

---

**Augmented design with intercept**

Introduce

$$
\widetilde X = \begin{bmatrix} \mathbf{1} & X \end{bmatrix} \in \mathbb{R}^{(M+N)\times 2},
\quad
\widetilde y = Y \in \mathbb{R}^{M+N},
$$

Define

$$
S = \widetilde X^\top \widetilde X
= \begin{pmatrix}
M + N & \sum_{i=1}^{M+N} X_i \\
\sum_{i=1}^{M+N} X_i & \sum_{i=1}^{M+N} X_i^2
\end{pmatrix},
\quad
t = \widetilde X^\top Y
= \begin{pmatrix}
\sum_{i=1}^{M+N} Y_i \\
\sum_{i=1}^{M+N} X_i Y_i
\end{pmatrix}.
$$

Then the OLS estimate $(\hat\alpha, \hat\beta)$ satisfies

$$
\begin{pmatrix} \hat\alpha \\ \beta \end{pmatrix}
= S^{-1} t,
$$

so

$$
\beta = e_2^\top S^{-1} t,
\quad \text{where } e_2 = (0, 1)^\top.
$$

---

**Shifting one block by $\delta$**

Observe that adding a constant $\delta$ to all entries of $Y_2$ does not change $\beta_1$ or $\beta_2$, but does change the pooled intercept and slope. Concretely, define the shift vector

$$
z_i = \begin{cases}
0, & i \leq M, \\
1, & M < i \leq M + N,
\end{cases}
\quad
z = (z_1, \dots, z_{M+N})^\top,
$$

and set

$$
Y' = Y + \delta z.
$$

Then

$$
t' = \widetilde X^\top Y' = t + \delta \widetilde X^\top z,
$$

and the new slope is

$$
\beta' = e_2^\top S^{-1} t' = e_2^\top S^{-1} t + \delta \, e_2^\top S^{-1} (\widetilde X^\top z)
= \beta + \delta c,
$$

where one checks

$$
c = e_2^\top S^{-1} (\widetilde X^\top z)
= \frac{M N (\bar X_2 - \bar X_1)}{\det S}.
$$

However, $c \neq 0$ under assumption (1) and nondegeneracy of $S$ (which follows by non-zero variance of $X$, i.e., assumption (2)). By choosing $\delta \to \pm\infty$ and by continuity,

$$
\beta' \quad \text{can be made to run over all of } \mathbb{R}.
$$

Therefore, even with $\beta_1$, $\beta_2$ fixed, the pooled-data OLS slope satisfies

$$
\boxed{ \beta \in (-\infty, \infty) }.
$$

````

```{code-cell} python
:tags: [hide-input, hide-output]
import numpy as np
import matplotlib.pyplot as plt

# Parameters

M = 20
N = 20
np.random.seed(42)

# Generate X1 and X2 with different means

X1 = np.random.normal(loc=0.0, scale=1.0, size=M)
X2 = np.random.normal(loc=5.0, scale=1.0, size=N)

# Generate Y1 and Y2 with known slopes + noise

Y1 = 2.0 *X1 + np.random.normal(0, 0.5, size=M)
Y2_base = -1.0* X2 + np.random.normal(0, 0.5, size=N)

# Function to compute and plot OLS fits

def plot_shifted_case(shift_delta, title_suffix):
    Y2_shifted = Y2_base + shift_delta
    X = np.concatenate([X1, X2])
    Y = np.concatenate([Y1, Y2_shifted])

    # Fit OLS for pooled data (with intercept)
    X_aug = np.column_stack([np.ones_like(X), X])
    beta_pooled = np.linalg.lstsq(X_aug, Y, rcond=None)[0]

    # Fit OLS for individual groups
    beta1 = np.linalg.lstsq(np.column_stack([np.ones_like(X1), X1]), Y1, rcond=None)[0]
    beta2 = np.linalg.lstsq(np.column_stack([np.ones_like(X2), X2]), Y2_shifted, rcond=None)[0]

    # Scatter plot of data
    plt.figure(figsize=(8, 6))
    plt.scatter(X1, Y1, color='green', label='Group 1 (Y1)', alpha=0.6)
    plt.scatter(X2, Y2_shifted, color='red', label='Group 2 (Y2 shifted)', alpha=0.6)

    # Plot OLS fits
    x_fit = np.linspace(-2, 8, 100)
    plt.plot(x_fit, beta1[0] + beta1[1] * x_fit, color='green', linestyle='--', label=f'OLS Group 1: slope={beta1[1]:.2f}')
    plt.plot(x_fit, beta2[0] + beta2[1] * x_fit, color='red', linestyle='--', label=f'OLS Group 2: slope={beta2[1]:.2f}')
    plt.plot(x_fit, beta_pooled[0] + beta_pooled[1] * x_fit, color='blue', linewidth=2, label=f'Pooled OLS: slope={beta_pooled[1]:.2f}')

    plt.title(f'Effect of Shift Î´={shift_delta} on Pooled Slope {title_suffix}')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.grid(True)
    plt.show()

# Case 1: Positive delta shift
plot_shifted_case(shift_delta=+20, title_suffix='(Positive shift)')

# Case 2: Negative delta shift
plot_shifted_case(shift_delta=-20, title_suffix='(Negative shift)')
```
