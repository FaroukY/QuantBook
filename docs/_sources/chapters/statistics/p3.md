---
jupytext:
  text_representation:
    format_name: myst
kernelspec:
  name: python3
  display_name: Python 3
---
# Problem 3: Episode III: Revenge of the Bias Guess

```{note}
You should attempt [](p1.md) and [](p2.md) before attempting this problem. 
```

Suppose you run two OLS regression models on disjoint datasets:

- One of $Y_1$ ($M \times 1$) onto $X_1$ ($M \times 1$),
- The other of $Y_2$ ($N \times 1$) onto $X_2$ ($N \times 1$).

Both models come **without intercept**.  Let the resulting coefficients be $\beta_1$ and $\beta_2$ respectively.

Now, suppose you co-fit another OLS model on the union of the two datasets (also without intercept).  

You therefore have a linear regression of $Y$ ($(M+N)\times 1$) onto $X$ ($(M+N)\times 1$), where $Y$ is a concatenation of $Y_1$ and $Y_2$, and $X$ is a concatenation of $X_1$ and $X_2$.

Assumptions:

- No intercepts are fitted in any of the three regressions.
- All $(x, y)$ pairs in $(X, Y)$ are drawn i.i.d. from a zero-mean 2D multivariate Gaussian distribution.

Given $\beta_1$, $\beta_2$, whatâ€™s your best guess at $\beta$ (the co-fitted slope)?

You may take reasonable approximations in your solution.

````{dropdown} Click to show solution

We have two disjoint datasets:

- First block: $Y_1 = \beta_1 X_1 + \varepsilon_1$, size $M$.
- Second block: $Y_2 = \beta_2 X_2 + \varepsilon_2$, size $N$.

Both regressions are without intercept.

The OLS estimator for each block is:

$$
\beta_1 = \frac{X_1^\top Y_1}{X_1^\top X_1},
\qquad
\beta_2 = \frac{X_2^\top Y_2}{X_2^\top X_2}.
$$

The pooled OLS slope (no intercept) is:

$$
\beta = \frac{X^\top Y}{X^\top X}
= \frac{X_1^\top Y_1 + X_2^\top Y_2}{X_1^\top X_1 + X_2^\top X_2}.
$$

Since:

$$
X_1^\top Y_1 = \beta_1 \, X_1^\top X_1,
\qquad
X_2^\top Y_2 = \beta_2 \, X_2^\top X_2,
$$

it follows that:

$$
\beta = \frac{ \beta_1 \, X_1^\top X_1 + \beta_2 \, X_2^\top X_2 }{ X_1^\top X_1 + X_2^\top X_2 }.
$$

By the law of large numbers:

$$
X_1^\top X_1 \approx M \, \sigma_x^2,
\quad
X_2^\top X_2 \approx N \, \sigma_x^2.
$$

Thus:

$$
\beta \approx \frac{ \beta_1 \, M \sigma_x^2 + \beta_2 \, N \sigma_x^2 }{ (M+N) \sigma_x^2 }
= \frac{M \, \beta_1 + N \, \beta_2}{M+N}.
$$

**Final Result:**

$$
\boxed{ \beta = \frac{M \, \beta_1 + N \, \beta_2}{M+N} \text{ as }M, N \to \infty}.
$$

````

```{code-cell} python
:tags: [hide-input, hide-output]
import numpy as np
import matplotlib.pyplot as plt

# Parameters
M = 250
N = 500
sigma_x = 1.0
sigma_y = 1.0
rho = 0.8

# Covariance matrix for bivariate normal
cov_matrix = np.array([[sigma_x**2, rho * sigma_x * sigma_y],
                       [rho * sigma_x * sigma_y, sigma_y**2]])

# Generate data
np.random.seed(42)
X1Y1 = np.random.multivariate_normal([0, 0], cov_matrix, size=M)
X2Y2 = np.random.multivariate_normal([0, 0], cov_matrix, size=N)

X1 = X1Y1[:, 0]
Y1 = X1Y1[:, 1]
X2 = X2Y2[:, 0]
Y2 = X2Y2[:, 1]

# Compute block OLS slopes (no intercept)
beta1 = (X1 @ Y1) / (X1 @ X1)
beta2 = (X2 @ Y2) / (X2 @ X2)

# Compute pooled OLS slope (no intercept)
X = np.concatenate([X1, X2])
Y = np.concatenate([Y1, Y2])
beta_pooled = (X @ Y) / (X @ X)

# Weighted average prediction
beta_weighted = (M * beta1 + N * beta2) / (M + N)

# Print results
print(f"Beta1 (block 1): {beta1:.4f}")
print(f"Beta2 (block 2): {beta2:.4f}")
print(f"Pooled OLS Beta: {beta_pooled:.4f}")
print(f"Approximated Beta (Our Answer): {beta_weighted:.4f}")

# Visualization
plt.figure(figsize=(8, 6))
plt.scatter(X1, Y1, alpha=0.6, label='Block 1', color='green')
plt.scatter(X2, Y2, alpha=0.6, label='Block 2', color='red')

x_fit = np.linspace(-3, 3, 100)
plt.plot(x_fit, beta1 * x_fit, linestyle='--', color='green', label=f'Fit Block 1: {beta1:.2f}')
plt.plot(x_fit, beta2 * x_fit, linestyle='--', color='red', label=f'Fit Block 2: {beta2:.2f}')
plt.plot(x_fit, beta_pooled * x_fit, linestyle='-', color='blue', linewidth=2, label=f'Pooled Fit: {beta_pooled:.2f}')

plt.xlabel('X')
plt.ylabel('Y')
plt.title('Verification of Pooled OLS as Weighted Average')
plt.legend()
plt.grid(True)
plt.show()
```
